
# ğŸš€ Big Data Technology â€“ Praktikum 1

**Setup Environment & Git Workflow**
Prodi Teknologi Informasi â€“ UIN Antasari
Lecturer: Muhayat, M.IT

---

## ğŸ“Œ Deskripsi Project

Praktikum ini membangun **fondasi environment Data Engineer modern** menggunakan stack industri:

* Python
* PySpark
* MongoDB Atlas (Cloud)
* Git & GitHub
* VS Code + PowerShell

Project ini mensimulasikan workflow profesional:
Local Development â†’ Distributed Processing â†’ Cloud Database â†’ Version Control

---

## ğŸ—ï¸ Technology Stack

### ğŸ’» VS Code

Development environment utama untuk coding dan integrasi terminal.

### ğŸ Python 3.10+

Bahasa utama untuk pemrosesan data dan PySpark.

### âš¡ PySpark

Engine distributed processing berbasis Apache Spark.

### â˜ï¸ MongoDB Atlas

Cloud NoSQL database (Free Tier M0).

### ğŸ” Git & GitHub

Version control dan repository online.

---

## ğŸ“‚ Struktur Project

```
bigdata-project/
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ cloud_storage/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ simple_job.py
â”‚   â””â”€â”€ test_mongo.py
â”œâ”€â”€ notebooks/
â”œâ”€â”€ reports/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Environment

### 1ï¸âƒ£ Install Dependency

```bash
pip install pyspark
pip install pymongo
```

Buat file `requirements.txt`:

```
pyspark
pymongo
```

Install sekaligus:

```bash
pip install -r requirements.txt
```

---

## ğŸ”¥ Menjalankan Spark Job

File: `scripts/simple_job.py`

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SimpleJob") \
    .getOrCreate()

data = [("A", 10), ("B", 20), ("A", 30)]
columns = ["category", "value"]

df = spark.createDataFrame(data, columns)
df.groupBy("category").sum("value").show()

spark.stop()
```

Jalankan:

```bash
python scripts/simple_job.py
```

Output yang diharapkan:

```
+--------+----------+
|category|sum(value)|
+--------+----------+
|       A|        40|
|       B|        20|
+--------+----------+
```

---

## â˜ï¸ Setup MongoDB Atlas

1. Buat cluster M0 Free Tier
2. Tambahkan Database User
3. Allow IP Address (0.0.0.0/0)
4. Ambil connection string
5. Test koneksi dengan:

```bash
python scripts/test_mongo.py
```

Jika berhasil, akan muncul:

```
Koneksi berhasil!
```

---

## ğŸ” Git Workflow

Inisialisasi repository:

```bash
git init
git add .
git commit -m "initial project setup"
```

Hubungkan ke GitHub:

```bash
git remote add origin <URL_REPOSITORY>
git branch -M main
git push -u origin main
```

---

## ğŸ“¸ Output Wajib Praktikum

Mahasiswa wajib mengumpulkan:

* Screenshot Spark berjalan
* Screenshot MongoDB Atlas (Status ACTIVE)
* Screenshot GitHub Repository
* File `simple_job.py`
* Screenshot hasil eksekusi Spark

---

## ğŸ§  Insight Praktikum

Praktikum ini bukan tentang analitik.

Tapi tentang:

* Environment reproducibility
* Cloud readiness
* Distributed computing mindset
* Professional Git workflow

Ini adalah fondasi sebelum masuk ke:

* ETL Pipeline
* Data Warehouse
* Streaming
* Machine Learning

---

## ğŸ¯ Status

âœ… PySpark Installed
âœ… MongoDB Connected
âœ… Git Initialized
âœ… Spark Job Running

---

## ğŸ‘©â€ğŸ’» Author

Mahmudah-TI23A
230104040220
Big Data Technology Track

